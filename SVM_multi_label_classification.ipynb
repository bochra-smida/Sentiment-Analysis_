{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\49163\\anaconda3\\lib\\site-packages (1.16.50)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.50 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from boto3) (1.19.50)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from boto3) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from botocore<1.20.0,>=1.19.50->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in c:\\users\\49163\\anaconda3\\lib\\site-packages (from botocore<1.20.0,>=1.19.50->boto3) (1.25.11)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.50->boto3) (1.15.0)\n",
      "Collecting pypads\n",
      "  Downloading pypads-0.5.7-py3-none-any.whl (228 kB)\n",
      "Collecting padre\n",
      "  Downloading padre-2.5.5.tar.gz (40 kB)\n",
      "Collecting jsonpath-rw<2.0.0,>=1.4.0\n",
      "  Downloading jsonpath-rw-1.4.0.tar.gz (13 kB)\n",
      "Collecting pymongo==3.11.0\n",
      "  Downloading pymongo-3.11.0-cp38-cp38-win_amd64.whl (382 kB)\n",
      "Collecting boltons<20.0.0,>=19.3.0\n",
      "  Downloading boltons-19.3.0-py2.py3-none-any.whl (166 kB)\n",
      "Collecting loguru<0.5.0,>=0.4.1\n",
      "  Downloading loguru-0.4.1-py3-none-any.whl (54 kB)\n",
      "Collecting pydantic<2.0.0,>=1.5.1\n",
      "  Downloading pydantic-1.7.3-cp38-cp38-win_amd64.whl (1.8 MB)\n",
      "Collecting mlflow<2.0.0,>=1.12.1\n",
      "  Downloading mlflow-1.13.1-py3-none-any.whl (14.1 MB)\n",
      "Requirement already satisfied: cloudpickle<2.0.0,>=1.3.0 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from pypads) (1.6.0)\n",
      "Collecting jsonpath-rw-ext<2.0.0,>=1.2.2\n",
      "  Downloading jsonpath_rw_ext-1.2.2-py2.py3-none-any.whl (19 kB)\n",
      "Collecting neural-fmri\n",
      "  Downloading neural-fmri-1.2.5.tar.gz (43 kB)\n",
      "Collecting pydicom\n",
      "  Downloading pydicom-2.1.2-py3-none-any.whl (1.9 MB)\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.0.tar.gz (48 kB)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\49163\\anaconda3\\lib\\site-packages (from padre) (3.0.5)\n",
      "Collecting gini\n",
      "  Downloading gini-0.7.tar.gz (5.1 kB)\n",
      "Requirement already satisfied: ply in c:\\users\\49163\\anaconda3\\lib\\site-packages (from jsonpath-rw<2.0.0,>=1.4.0->pypads) (3.11)\n",
      "Requirement already satisfied: decorator in c:\\users\\49163\\anaconda3\\lib\\site-packages (from jsonpath-rw<2.0.0,>=1.4.0->pypads) (4.4.2)\n",
      "Requirement already satisfied: six in c:\\users\\49163\\anaconda3\\lib\\site-packages (from jsonpath-rw<2.0.0,>=1.4.0->pypads) (1.15.0)\n",
      "Requirement already satisfied: colorama>=0.3.4; sys_platform == \"win32\" in c:\\users\\49163\\anaconda3\\lib\\site-packages (from loguru<0.5.0,>=0.4.1->pypads) (0.4.4)\n",
      "Collecting win32-setctime>=1.0.0; sys_platform == \"win32\"\n",
      "  Downloading win32_setctime-1.0.3-py3-none-any.whl (3.5 kB)\n",
      "Collecting waitress; platform_system == \"Windows\"\n",
      "  Downloading waitress-1.4.4-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: Flask in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (1.1.2)\n",
      "Collecting docker>=4.0.0\n",
      "  Downloading docker-4.4.1-py2.py3-none-any.whl (146 kB)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.12-py3-none-any.whl (159 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (1.1.3)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (7.1.2)\n",
      "Collecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting azure-storage-blob>=12.0.0\n",
      "  Downloading azure_storage_blob-12.7.0-py2.py3-none-any.whl (339 kB)\n",
      "Collecting alembic<=1.4.1\n",
      "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
      "Requirement already satisfied: requests>=2.17.3 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (2.24.0)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.18.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (1.3.20)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.14.1.tar.gz (54 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (3.14.0)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (0.4.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (5.3.1)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\49163\\anaconda3\\lib\\site-packages (from mlflow<2.0.0,>=1.12.1->pypads) (2.8.1)\n",
      "Collecting pbr>=1.8\n",
      "  Downloading pbr-5.5.1-py2.py3-none-any.whl (106 kB)\n",
      "Requirement already satisfied: chardet in c:\\users\\49163\\anaconda3\\lib\\site-packages (from neural-fmri->padre) (3.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\49163\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\49163\\\\AppData\\\\Local\\\\Temp\\\\pip-install-in41b1my\\\\python-levenshtein\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\49163\\\\AppData\\\\Local\\\\Temp\\\\pip-install-in41b1my\\\\python-levenshtein\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\49163\\AppData\\Local\\Temp\\pip-wheel-sbq4xyoc'\n",
      "       cwd: C:\\Users\\49163\\AppData\\Local\\Temp\\pip-install-in41b1my\\python-levenshtein\\\n",
      "  Complete output (27 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  copying Levenshtein\\StringMatcher.py -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  copying Levenshtein\\__init__.py -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  running egg_info\n",
      "  writing python_Levenshtein.egg-info\\PKG-INFO\n",
      "  writing dependency_links to python_Levenshtein.egg-info\\dependency_links.txt\n",
      "  writing entry points to python_Levenshtein.egg-info\\entry_points.txt\n",
      "  writing namespace_packages to python_Levenshtein.egg-info\\namespace_packages.txt\n",
      "  writing requirements to python_Levenshtein.egg-info\\requires.txt\n",
      "  writing top-level names to python_Levenshtein.egg-info\\top_level.txt\n",
      "  reading manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*pyc' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*so' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.project' found anywhere in distribution\n",
      "  warning: no previously-included files matching '.pydevproject' found anywhere in distribution\n",
      "  writing manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\n",
      "  copying Levenshtein\\_levenshtein.c -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  copying Levenshtein\\_levenshtein.h -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "  running build_ext\n",
      "  building 'Levenshtein._levenshtein' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for python-Levenshtein\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\49163\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\49163\\\\AppData\\\\Local\\\\Temp\\\\pip-install-in41b1my\\\\python-levenshtein\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\49163\\\\AppData\\\\Local\\\\Temp\\\\pip-install-in41b1my\\\\python-levenshtein\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\49163\\AppData\\Local\\Temp\\pip-record-pneeojrj\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\49163\\Anaconda3\\Include\\python-Levenshtein'\n",
      "         cwd: C:\\Users\\49163\\AppData\\Local\\Temp\\pip-install-in41b1my\\python-levenshtein\\\n",
      "    Complete output (27 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.8\n",
      "    creating build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    copying Levenshtein\\StringMatcher.py -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    copying Levenshtein\\__init__.py -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    running egg_info\n",
      "    writing python_Levenshtein.egg-info\\PKG-INFO\n",
      "    writing dependency_links to python_Levenshtein.egg-info\\dependency_links.txt\n",
      "    writing entry points to python_Levenshtein.egg-info\\entry_points.txt\n",
      "    writing namespace_packages to python_Levenshtein.egg-info\\namespace_packages.txt\n",
      "    writing requirements to python_Levenshtein.egg-info\\requires.txt\n",
      "    writing top-level names to python_Levenshtein.egg-info\\top_level.txt\n",
      "    reading manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no previously-included files matching '*pyc' found anywhere in distribution\n",
      "    warning: no previously-included files matching '*so' found anywhere in distribution\n",
      "    warning: no previously-included files matching '.project' found anywhere in distribution\n",
      "    warning: no previously-included files matching '.pydevproject' found anywhere in distribution\n",
      "    writing manifest file 'python_Levenshtein.egg-info\\SOURCES.txt'\n",
      "    copying Levenshtein\\_levenshtein.c -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    copying Levenshtein\\_levenshtein.h -> build\\lib.win-amd64-3.8\\Levenshtein\n",
      "    running build_ext\n",
      "    building 'Levenshtein._levenshtein' extension\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\49163\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\49163\\\\AppData\\\\Local\\\\Temp\\\\pip-install-in41b1my\\\\python-levenshtein\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\49163\\\\AppData\\\\Local\\\\Temp\\\\pip-install-in41b1my\\\\python-levenshtein\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\49163\\AppData\\Local\\Temp\\pip-record-pneeojrj\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\49163\\Anaconda3\\Include\\python-Levenshtein' Check the logs for full command output."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nibabel\n",
      "  Downloading nibabel-3.2.1-py3-none-any.whl (3.3 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\49163\\anaconda3\\lib\\site-packages (from python-Levenshtein->padre) (50.3.1.post20201107)\n",
      "Requirement already satisfied: jdcal in c:\\users\\49163\\anaconda3\\lib\\site-packages (from openpyxl->padre) (1.4.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\49163\\anaconda3\\lib\\site-packages (from openpyxl->padre) (1.0.1)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from Flask->mlflow<2.0.0,>=1.12.1->pypads) (1.0.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from Flask->mlflow<2.0.0,>=1.12.1->pypads) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from Flask->mlflow<2.0.0,>=1.12.1->pypads) (2.11.2)\n",
      "Requirement already satisfied: pywin32==227; sys_platform == \"win32\" in c:\\users\\49163\\anaconda3\\lib\\site-packages (from docker>=4.0.0->mlflow<2.0.0,>=1.12.1->pypads) (227)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from pandas->mlflow<2.0.0,>=1.12.1->pypads) (2020.1)\n",
      "Collecting azure-core<2.0.0,>=1.10.0\n",
      "  Downloading azure_core-1.10.0-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from azure-storage-blob>=12.0.0->mlflow<2.0.0,>=1.12.1->pypads) (3.1.1)\n",
      "Collecting msrest>=0.6.10\n",
      "  Downloading msrest-0.6.19-py2.py3-none-any.whl (84 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.4.tar.gz (479 kB)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow<2.0.0,>=1.12.1->pypads) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow<2.0.0,>=1.12.1->pypads) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow<2.0.0,>=1.12.1->pypads) (2020.6.20)\n",
      "Requirement already satisfied: prometheus_client in c:\\users\\49163\\anaconda3\\lib\\site-packages (from prometheus-flask-exporter->mlflow<2.0.0,>=1.12.1->pypads) (0.8.0)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from nibabel->neural-fmri->padre) (20.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->Flask->mlflow<2.0.0,>=1.12.1->pypads) (1.1.1)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<2.0.0,>=1.12.1->pypads) (1.14.3)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from msrest>=0.6.10->azure-storage-blob>=12.0.0->mlflow<2.0.0,>=1.12.1->pypads) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from packaging>=14.3->nibabel->neural-fmri->padre) (2.4.7)\n",
      "Requirement already satisfied: pycparser in c:\\users\\49163\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<2.0.0,>=1.12.1->pypads) (2.20)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\49163\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.10->azure-storage-blob>=12.0.0->mlflow<2.0.0,>=1.12.1->pypads) (3.1.0)\n",
      "Building wheels for collected packages: padre, jsonpath-rw, neural-fmri, python-Levenshtein, gini, alembic, prometheus-flask-exporter, databricks-cli, Mako\n",
      "  Building wheel for padre (setup.py): started\n",
      "  Building wheel for padre (setup.py): finished with status 'done'\n",
      "  Created wheel for padre: filename=padre-2.5.5-py3-none-any.whl size=50345 sha256=47c148a04765edc9b54f7d79d8ac098a49a30d614c50a0331dbae939f9e27c8b\n",
      "  Stored in directory: c:\\users\\49163\\appdata\\local\\pip\\cache\\wheels\\af\\96\\ae\\763252e1238bdf4aa2b5fa697dcd0fd1153578afe8506935cd\n",
      "  Building wheel for jsonpath-rw (setup.py): started\n",
      "  Building wheel for jsonpath-rw (setup.py): finished with status 'done'\n",
      "  Created wheel for jsonpath-rw: filename=jsonpath_rw-1.4.0-py3-none-any.whl size=15151 sha256=096f8db48b6f3477c39c7150d4c10be0ae50204f3dbd1ca7e7f80e608ed1e26a\n",
      "  Stored in directory: c:\\users\\49163\\appdata\\local\\pip\\cache\\wheels\\2b\\f3\\d4\\2f6fb63766f0479b061d03bab249bbd44f116ae5c73b9f8a24\n",
      "  Building wheel for neural-fmri (setup.py): started\n",
      "  Building wheel for neural-fmri (setup.py): finished with status 'done'\n",
      "  Created wheel for neural-fmri: filename=neural_fmri-1.2.5-py3-none-any.whl size=51346 sha256=353617b091445e4a6c44aeee34973aa27070379658ef92efc0737c1c2df8a39a\n",
      "  Stored in directory: c:\\users\\49163\\appdata\\local\\pip\\cache\\wheels\\73\\a4\\49\\61d6c56167a83205d86281a7bca00a1e0d744850d630185300\n",
      "  Building wheel for python-Levenshtein (setup.py): started\n",
      "  Building wheel for python-Levenshtein (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for python-Levenshtein\n",
      "  Building wheel for gini (setup.py): started\n",
      "  Building wheel for gini (setup.py): finished with status 'done'\n",
      "  Created wheel for gini: filename=gini-0.7-py3-none-any.whl size=6271 sha256=a512b9d0c58f51be1389b6b10ae5e91b62734b57e91de64bc60c13f889a0ef22\n",
      "  Stored in directory: c:\\users\\49163\\appdata\\local\\pip\\cache\\wheels\\9a\\39\\6d\\3ca8c4d66c04bf7b15f9495a4b309263588e3a5e02c1961f17\n",
      "  Building wheel for alembic (setup.py): started\n",
      "  Building wheel for alembic (setup.py): finished with status 'done'\n",
      "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158162 sha256=047f516eee487f7dce06ace703ecedae2c0efbfaaecd8f59e093c9f85e64547e\n",
      "  Stored in directory: c:\\users\\49163\\appdata\\local\\pip\\cache\\wheels\\9d\\de\\6d\\ca8d461ec29e010b1267d7353d0b058819770f7680bb9360e4\n",
      "  Building wheel for prometheus-flask-exporter (setup.py): started\n",
      "  Building wheel for prometheus-flask-exporter (setup.py): finished with status 'done'\n",
      "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-py3-none-any.whl size=17162 sha256=507f96c738b1b57305c8d78ed6c89b7033a5920e006a12587d3419909aff5505\n",
      "  Stored in directory: c:\\users\\49163\\appdata\\local\\pip\\cache\\wheels\\12\\1a\\8d\\0c016e06370d07f82def661b6cb7d91d4e6b4ff7f2982e9f2c\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.14.1-py3-none-any.whl size=100583 sha256=d9ebdfceabdc0ab87a022f86b2658756a6d386cd06ce32d59eba894a22b06339\n",
      "  Stored in directory: c:\\users\\49163\\appdata\\local\\pip\\cache\\wheels\\f8\\a7\\7c\\74d614edb0dea04c3bb450951ec35c8f62aa197302ad2c3baa\n",
      "  Building wheel for Mako (setup.py): started\n",
      "  Building wheel for Mako (setup.py): finished with status 'done'\n",
      "  Created wheel for Mako: filename=Mako-1.1.4-py2.py3-none-any.whl size=75681 sha256=eec7960a9e7d9d50922f61d19264adb6862ee22942700f375cd566cafdb2c108\n",
      "  Stored in directory: c:\\users\\49163\\appdata\\local\\pip\\cache\\wheels\\17\\bc\\50\\621fe4100d907a7296cc00c21371402b068b648820f6ff5812\n",
      "Successfully built padre jsonpath-rw neural-fmri gini alembic prometheus-flask-exporter databricks-cli Mako\n",
      "Failed to build python-Levenshtein\n",
      "Installing collected packages: jsonpath-rw, pymongo, boltons, win32-setctime, loguru, pydantic, waitress, websocket-client, docker, smmap, gitdb, gitpython, querystring-parser, azure-core, isodate, msrest, azure-storage-blob, Mako, python-editor, alembic, prometheus-flask-exporter, tabulate, databricks-cli, mlflow, pbr, jsonpath-rw-ext, pypads, pydicom, fuzzywuzzy, python-Levenshtein, nibabel, neural-fmri, gini, padre\n",
      "    Running setup.py install for python-Levenshtein: started\n",
      "    Running setup.py install for python-Levenshtein: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 \n",
    "!pip install pypads padre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypads.app.base import PyPads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\49163\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import datetime\n",
    "import dash_core_components as dcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe size\n",
      "Rows: 6838\n",
      "Columns: 12\n",
      "             anger  anticipation      disgust         fear          joy  \\\n",
      "count  6838.000000   6838.000000  6838.000000  6838.000000  6838.000000   \n",
      "mean      0.372039      0.143024     0.380521     0.181632     0.362240   \n",
      "std       0.483384      0.350123     0.485550     0.385569     0.480683   \n",
      "min       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
      "75%       1.000000      0.000000     1.000000     0.000000     1.000000   \n",
      "max       1.000000      1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "              love     optimism    pessimism      sadness     surprise  \\\n",
      "count  6838.000000  6838.000000  6838.000000  6838.000000  6838.000000   \n",
      "mean      0.102369     0.290143     0.116262     0.293653     0.052793   \n",
      "std       0.303155     0.453862     0.320562     0.455468     0.223637   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     1.000000     0.000000     1.000000     0.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "             trust  \n",
      "count  6838.000000  \n",
      "mean      0.052208  \n",
      "std       0.222463  \n",
      "min       0.000000  \n",
      "25%       0.000000  \n",
      "50%       0.000000  \n",
      "75%       0.000000  \n",
      "max       1.000000  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“worry is a down payment on a problem you may ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>whatever you decide to do make sure it makes y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_kellerman  it also helps that the majority of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accept the challenges so that you can literall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my roommate: it's okay that we can't spell bec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  anger  anticipation  \\\n",
       "0  “worry is a down payment on a problem you may ...      0             1   \n",
       "1  whatever you decide to do make sure it makes y...      0             0   \n",
       "2  _kellerman  it also helps that the majority of...      1             0   \n",
       "3  accept the challenges so that you can literall...      0             0   \n",
       "4  my roommate: it's okay that we can't spell bec...      1             0   \n",
       "\n",
       "   disgust  fear  joy  love  optimism  pessimism  sadness  surprise  trust  \n",
       "0        0     0    0     0         1          0        0         0      1  \n",
       "1        0     0    1     1         1          0        0         0      0  \n",
       "2        1     0    1     0         1          0        0         0      0  \n",
       "3        0     0    1     0         1          0        0         0      0  \n",
       "4        1     0    0     0         0          0        0         0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading and working train data (the train_data was loaded and and the tweet texts were cleaned in another code)\n",
    "train_data =pd.read_csv(r\"C:\\Users\\49163\\Desktop\\Uni Passau\\Data science\\bert_train_df.csv\")\n",
    "rows, columns = train_data.shape\n",
    "print('Dataframe size')\n",
    "print('Rows:', rows)\n",
    "print('Columns:', columns)\n",
    "print(train_data.describe())\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5470,)\n",
      "(1368,)\n"
     ]
    }
   ],
   "source": [
    "#splitting the data into train and test\n",
    "#The split is done beforehand\n",
    "categories = [\"anger\",\"anticipation\", \"disgust\", \"fear\", \"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]\n",
    "train, test = train_test_split(train_data, random_state=42, test_size=0.20, shuffle=True)\n",
    "X_train = train.text\n",
    "X_test = test.text\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize output labels\n",
    "\n",
    "\n",
    "L=[]\n",
    "for i in range(len(train_data)):\n",
    "    S=\"\"\n",
    "    for j in [\"text\",\"anger\",\"anticipation\", \"disgust\", \"fear\", \"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]:\n",
    "        if train_data.loc[i,j]=='1':\n",
    "            S= S+\" , \"+j[:3]\n",
    "    L.append(S)\n",
    "train_data[\"label\"]=L\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiClass SVM\n",
    "Reducing multiclass problems to multiple binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # X_train is the text, category contains the sentiments and whether it exists or not\n",
    "    SVC_pipeline.fit(X_train, train[category])\n",
    "    # calculate the test accuracy\n",
    "    prediction = SVC_pipeline.predict(X_test)\n",
    "    \n",
    "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predicting on our dataset:\n",
    "\n",
    "corona_tweets = pd.read_csv(r'C:\\Users\\49163\\Downloads\\tweets copy\\tweets copy\\London_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "      <th>tweet_lang</th>\n",
       "      <th>source</th>\n",
       "      <th>place</th>\n",
       "      <th>geom</th>\n",
       "      <th>retweets</th>\n",
       "      <th>tweet_favorites</th>\n",
       "      <th>...</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>user_favorites</th>\n",
       "      <th>status</th>\n",
       "      <th>user_lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1221265292363517952</td>\n",
       "      <td>2020-01-26 02:55:10</td>\n",
       "      <td>Wakeyyyyy wakeyyyy world....ohhhh Petra and Ma...</td>\n",
       "      <td>ausopen</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Hammersmith, London</td>\n",
       "      <td>0103000020E61000000100000005000000AE62F19BC24A...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Bparlma</td>\n",
       "      <td>London</td>\n",
       "      <td>2508</td>\n",
       "      <td>2213</td>\n",
       "      <td>77383</td>\n",
       "      <td>447423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1221265334478561280</td>\n",
       "      <td>2020-01-26 02:55:20</td>\n",
       "      <td>@JohnK68266141 @Saffiya_Khan1 @Spoonhead8 @Lit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Wandsworth, London</td>\n",
       "      <td>0103000020E6100000010000000500000097395D16139B...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>timspencer1</td>\n",
       "      <td>London</td>\n",
       "      <td>2463</td>\n",
       "      <td>3004</td>\n",
       "      <td>74206</td>\n",
       "      <td>56556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1221265364060864513</td>\n",
       "      <td>2020-01-26 02:55:27</td>\n",
       "      <td>I think I should consider bandaging my elbow a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Tottenham, London</td>\n",
       "      <td>0103000020E610000001000000050000002C2D23F59ECA...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>carddcaptor</td>\n",
       "      <td>London, England</td>\n",
       "      <td>1969</td>\n",
       "      <td>244</td>\n",
       "      <td>7139</td>\n",
       "      <td>65061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1221265413306290176</td>\n",
       "      <td>2020-01-26 02:55:39</td>\n",
       "      <td>Sono a ballare e delle ragazze ad una certa si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Camberwell, London</td>\n",
       "      <td>0103000020E6100000010000000500000017F549EEB089...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>martinamrcss</td>\n",
       "      <td>London, England</td>\n",
       "      <td>247</td>\n",
       "      <td>173</td>\n",
       "      <td>11667</td>\n",
       "      <td>9737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1221265416405835776</td>\n",
       "      <td>2020-01-26 02:55:39</td>\n",
       "      <td>I swear everytime I’ve seen this pic I think i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Brent, London</td>\n",
       "      <td>0103000020E61000000100000005000000E010AAD4EC81...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>waliidrashiid</td>\n",
       "      <td>London, England</td>\n",
       "      <td>416</td>\n",
       "      <td>238</td>\n",
       "      <td>19934</td>\n",
       "      <td>17451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            message_id                 date  \\\n",
       "0  1221265292363517952  2020-01-26 02:55:10   \n",
       "1  1221265334478561280  2020-01-26 02:55:20   \n",
       "2  1221265364060864513  2020-01-26 02:55:27   \n",
       "3  1221265413306290176  2020-01-26 02:55:39   \n",
       "4  1221265416405835776  2020-01-26 02:55:39   \n",
       "\n",
       "                                                text     tags tweet_lang  \\\n",
       "0  Wakeyyyyy wakeyyyy world....ohhhh Petra and Ma...  ausopen         en   \n",
       "1  @JohnK68266141 @Saffiya_Khan1 @Spoonhead8 @Lit...      NaN         en   \n",
       "2  I think I should consider bandaging my elbow a...      NaN         en   \n",
       "3  Sono a ballare e delle ragazze ad una certa si...      NaN         it   \n",
       "4  I swear everytime I’ve seen this pic I think i...      NaN         en   \n",
       "\n",
       "                                              source                place  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...  Hammersmith, London   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...   Wandsworth, London   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...    Tottenham, London   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...   Camberwell, London   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...        Brent, London   \n",
       "\n",
       "                                                geom  retweets  \\\n",
       "0  0103000020E61000000100000005000000AE62F19BC24A...         0   \n",
       "1  0103000020E6100000010000000500000097395D16139B...         0   \n",
       "2  0103000020E610000001000000050000002C2D23F59ECA...         0   \n",
       "3  0103000020E6100000010000000500000017F549EEB089...         0   \n",
       "4  0103000020E61000000100000005000000E010AAD4EC81...         0   \n",
       "\n",
       "   tweet_favorites  ...      user_name    user_location  followers friends  \\\n",
       "0                0  ...        Bparlma           London       2508    2213   \n",
       "1                0  ...    timspencer1           London       2463    3004   \n",
       "2                0  ...    carddcaptor  London, England       1969     244   \n",
       "3                0  ...   martinamrcss  London, England        247     173   \n",
       "4                0  ...  waliidrashiid  London, England        416     238   \n",
       "\n",
       "  user_favorites  status  user_lang  latitude  longitude  data_source  \n",
       "0          77383  447423        NaN       NaN        NaN          [3]  \n",
       "1          74206   56556        NaN       NaN        NaN          [3]  \n",
       "2           7139   65061        NaN       NaN        NaN          [3]  \n",
       "3          11667    9737        NaN       NaN        NaN          [3]  \n",
       "4          19934   17451        NaN       NaN        NaN          [3]  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect\n",
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Translating all the tweets to English using langdetect \n",
    "  \n",
    "from langdetect import detect \n",
    "from googletrans import Translator  \n",
    "\n",
    "\n",
    "translator = Translator()\n",
    "for i in range (len(corona_tweets)):\n",
    "    text=corona_tweets.loc[i,\"text\"]\n",
    "    try:\n",
    "        corona_tweets.loc[i,\"text\"] = translator.translate(text)\n",
    "    except:\n",
    "        text=None\n",
    "        \n",
    "    \n",
    "\n",
    "corona_tweets.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-425d08ba9cd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorona_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mcorona_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_emoji\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorona_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#print(type(corona_tweets['text'][0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   1763\u001b[0m                 \u001b[1;31m# scalar value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mloc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0milocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1765\u001b[1;33m                     \u001b[0misetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36misetter\u001b[1;34m(loc, v)\u001b[0m\n\u001b[0;32m   1693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m                 \u001b[1;31m# reset the sliced object if unique\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iset_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m             \u001b[1;31m# we need an iterable, with a ndim of at least 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_iset_item\u001b[1;34m(self, loc, value)\u001b[0m\n\u001b[0;32m   3094\u001b[0m         \u001b[1;31m# technically _sanitize_column expects a label, not a position,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3095\u001b[0m         \u001b[1;31m#  but the behavior is the same as long as we pass broadcast=False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3096\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3097\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iset_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3741\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreindexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3743\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#cleaning the dataset\n",
    "import re\n",
    "def clean(t):\n",
    "    t = re.sub('@[A-Za-z0–9]+', '',str(t))\n",
    "    t = re.sub('#', '', str(t)) \n",
    "    t = re.sub('RT[\\s]+', '', str(t))\n",
    "    t= re.sub('<[^<]+?>', '', str(t))\n",
    "    t = re.sub('https?:\\/\\/\\S+', '', str(t))\n",
    "    t= re.sub(r'^https?://.*[rn]*', '',str(t))\n",
    "    return t\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "for i in range(len(corona_tweets)):\n",
    "    \n",
    "    corona_tweets.loc[i,\"text\"] = remove_emoji(corona_tweets.loc[i,\"text\"])\n",
    "\n",
    "#print(type(corona_tweets['text'][0]))\n",
    "corona_tweets['text'] = corona_tweets['text'].apply(clean)\n",
    "corona_tweets['text'] = corona_tweets['text'].str.lower()\n",
    "\n",
    "\n",
    "X = corona_tweets.text\n",
    "#X=vectorizer.transform(X)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing anger\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-78201f57b016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mSVC_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# compute the testing accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mcorona_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model\n",
    "    SVC_pipeline.fit(X_train, train[category])\n",
    "    #testing accuracy\n",
    "    prediction = SVC_pipeline.predict(X)\n",
    "    corona_tweets[category]=prediction\n",
    "    #print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>place</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-14 17:15:00</td>\n",
       "      <td>precis som förra gången hamnade vi hos grabbse...</td>\n",
       "      <td>Stockholm, Sverige</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-14 17:28:36</td>\n",
       "      <td>det är fortfarande svårt att prata om övergrep...</td>\n",
       "      <td>Stockholm, Sweden</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-14 17:42:03</td>\n",
       "      <td>when one can not go to the gym due to bad head...</td>\n",
       "      <td>Stockholm, Sweden</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-14 17:53:15</td>\n",
       "      <td>today sthlmtech turns 7 years old. massive tha...</td>\n",
       "      <td>Hilton Stockholm Slussen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-14 17:58:03</td>\n",
       "      <td>... and there is my friend alex  up talking se...</td>\n",
       "      <td>Hilton Stockholm Slussen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-14 18:01:10</td>\n",
       "      <td>a rainy and windy tuesday in january is perfec...</td>\n",
       "      <td>Stockholm, Sweden</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-14 18:15:28</td>\n",
       "      <td>”vi behöver bopol lösningar som sätter hyresgä...</td>\n",
       "      <td>Hökarängen T-bana</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-14 18:16:27</td>\n",
       "      <td>sinnessjukt bra pris på julmust just nu.\\nniel...</td>\n",
       "      <td>Stockholm, Sverige</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-14 18:17:06</td>\n",
       "      <td>i'm at ki-mama in stockholm, sweden</td>\n",
       "      <td>Stockholm, Sweden</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-14 18:28:59</td>\n",
       "      <td>welcome to the family  @ erik och izze's palats</td>\n",
       "      <td>Stockholm, Sverige</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date                                               text  \\\n",
       "0  2020-01-14 17:15:00  precis som förra gången hamnade vi hos grabbse...   \n",
       "1  2020-01-14 17:28:36  det är fortfarande svårt att prata om övergrep...   \n",
       "2  2020-01-14 17:42:03  when one can not go to the gym due to bad head...   \n",
       "3  2020-01-14 17:53:15  today sthlmtech turns 7 years old. massive tha...   \n",
       "4  2020-01-14 17:58:03  ... and there is my friend alex  up talking se...   \n",
       "5  2020-01-14 18:01:10  a rainy and windy tuesday in january is perfec...   \n",
       "6  2020-01-14 18:15:28  ”vi behöver bopol lösningar som sätter hyresgä...   \n",
       "7  2020-01-14 18:16:27  sinnessjukt bra pris på julmust just nu.\\nniel...   \n",
       "8  2020-01-14 18:17:06               i'm at ki-mama in stockholm, sweden    \n",
       "9  2020-01-14 18:28:59   welcome to the family  @ erik och izze's palats    \n",
       "\n",
       "                      place  anger  anticipation  disgust  fear  joy  love  \\\n",
       "0        Stockholm, Sverige      0             0        1     0    0     0   \n",
       "1         Stockholm, Sweden      0             0        0     0    0     0   \n",
       "2         Stockholm, Sweden      0             0        0     0    0     0   \n",
       "3  Hilton Stockholm Slussen      0             0        1     0    1     0   \n",
       "4  Hilton Stockholm Slussen      0             0        0     0    0     0   \n",
       "5         Stockholm, Sweden      0             0        0     0    1     0   \n",
       "6         Hökarängen T-bana      0             0        0     0    0     0   \n",
       "7        Stockholm, Sverige      1             0        0     0    0     0   \n",
       "8         Stockholm, Sweden      0             0        0     0    0     0   \n",
       "9        Stockholm, Sverige      0             0        0     0    1     0   \n",
       "\n",
       "   optimism  pessimism  sadness  surprise  trust  \n",
       "0         1          0        0         0      0  \n",
       "1         0          0        0         0      0  \n",
       "2         0          0        1         0      0  \n",
       "3         0          0        0         0      0  \n",
       "4         0          0        0         0      0  \n",
       "5         0          0        0         0      0  \n",
       "6         0          0        0         0      0  \n",
       "7         0          0        0         0      0  \n",
       "8         0          0        0         0      0  \n",
       "9         1          0        0         0      0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_pred=corona_tweets[[\"date\",\"text\",\"place\",\"anger\",\"anticipation\", \"disgust\", \"fear\", \"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]]\n",
    "\n",
    "corona_pred.to_csv(r\"C:\\Users\\49163\\Desktop\\Uni Passau\\Data science\\archive\\stockholm_multilabel_svc_pred.csv\")\n",
    "corona_pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df =tracker.results.get_experiments_data_frame(\"Data Science LabGroup 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
